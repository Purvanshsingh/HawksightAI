import asyncio
import os
from typing import Any, Dict

from tools.data_tools import (
    create_datasets,
    profile_data,
    detect_anomalies_and_drift,
    detect_pii,
    sync_tables,
    generate_lineage_graph,
    compile_report,
    save_governance_report,
)
from agents import create_llm_eagle_agent, create_eagle_agent

# Optional ADK / GenAI imports so the script can still run when packages are missing
try:
    from google.adk.runners import Runner
    from google.adk.sessions import InMemorySessionService
    from google.genai import types as genai_types

    ADK_AVAILABLE = True
except ImportError as e:  # pragma: no cover - guard for missing deps
    ADK_AVAILABLE = False
    ADK_IMPORT_ERROR = e


async def run_demo(baseline_path: str, current_path: str, config: Dict[str, Any], use_llm: bool = False) -> None:
    """Run either the LLM agent pipeline or a deterministic fallback."""
    if not ADK_AVAILABLE:
        print(
            "Google ADK / GenAI packages are not installed. "
            "Install them (e.g., `pip install google-adk google-genai`) to run agents.\n"
            f"Import error: {ADK_IMPORT_ERROR}"
        )
        return

    # Ensure data exists
    data_dir = os.path.dirname(baseline_path)
    if not os.path.isfile(baseline_path) or not os.path.isfile(current_path):
        print(f"Dataset(s) not found in {data_dir}. Generating synthetic data...")
        create_datasets(base_dir=data_dir, n=500)

    baseline_profile = profile_data(baseline_path)
    print("Baseline profile computed.")

    # LLM config from config.json
    llm_cfg: Dict[str, Any] = config.get("llm", {}) if isinstance(config, dict) else {}
    api_key = llm_cfg.get("google_api_key")
    model_name = llm_cfg.get("model_name", "gemini-2.0-flash")

    llm_enabled = bool(use_llm)
    if llm_enabled and api_key:
        os.environ["GOOGLE_API_KEY"] = api_key
        os.environ.setdefault("GOOGLE_GENAI_USE_VERTEXAI", "FALSE")
    else:
        if use_llm and not api_key:
            print("LLM explicitly requested but no GOOGLE_API_KEY provided; falling back to deterministic agents.")
        elif not use_llm:
            print("LLM disabled (default). Running deterministic agents without network calls.")
        llm_enabled = False

    def run_deterministic_pipeline() -> str:
        """Local, non-LLM pipeline so we always produce a report when LLM fails."""
        current_profile = profile_data(current_path)
        anomalies = detect_anomalies_and_drift(current_path, baseline_profile)
        compliance = detect_pii(current_path)
        cleaned = sync_tables(current_path)
        lineage = generate_lineage_graph()
        report = compile_report(
            profile=current_profile,
            anomalies=anomalies,
            compliance_issues=compliance,
            cleaned_path=cleaned,
            lineage=lineage,
        )
        return save_governance_report(report)

    # If LLM is disabled, run deterministic and exit
    if not llm_enabled:
        report_path = run_deterministic_pipeline()
        print(f"Deterministic governance report written to: {report_path}")
        return

    # Session service + async session
    session_service = InMemorySessionService()
    session = await session_service.create_session(
        app_name="agents",
        user_id="demo_user",
        session_id="demo_session",
        state={"baseline_profile": baseline_profile},
    )

    # Build root agent
    try:
        eagle = create_llm_eagle_agent(model=model_name)
    except Exception as e:
        print(f"Failed to create LLM eagle agent, falling back to deterministic: {e}")
        eagle = create_eagle_agent()

    runner = Runner(agent=eagle, session_service=session_service, app_name="agents")

    content = genai_types.Content(
        role="user",
        parts=[genai_types.Part(text=f"Analyze data at: {current_path}")],
    )

    try:
        events = runner.run(user_id="demo_user", session_id=session.id, new_message=content)
        # Consume and log the event stream so sub-agents execute and we can spot failures
        for ev in events:
            print("Event:", ev)
    except Exception as e:
        print(f"Runner failed while executing agents: {e}")
        return

    report_path = session.state.get("report_path")
    if report_path:
        print(f"Governance report written to: {report_path}")
    else:
        print("No report was generated by LLM agents; falling back to deterministic pipeline.")
        fallback_path = run_deterministic_pipeline()
        print(f"Deterministic governance report written to: {fallback_path}")


def main():
    import argparse
    import json

    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json")
    parser.add_argument("--baseline")
    parser.add_argument("--current")
    parser.add_argument("--use-llm", action="store_true", help="Enable LLM agents (requires GOOGLE_API_KEY).")
    args = parser.parse_args()

    with open(args.config, "r", encoding="utf-8") as f:
        config = json.load(f)

    base_dir = os.path.dirname(args.config)
    baseline_path = args.baseline or os.path.join(
        base_dir, config.get("data", {}).get("baseline_file", "data/transactions_baseline.csv")
    )
    current_path = args.current or os.path.join(
        base_dir, config.get("data", {}).get("current_file", "data/transactions_current.csv")
    )

    asyncio.run(run_demo(baseline_path, current_path, config, use_llm=args.use_llm))


if __name__ == "__main__":
    main()
