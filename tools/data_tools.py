"""Custom data tools for HawkSightAI.

This module defines all of the functions used by the agents.  The
functions are deliberately simple so the demo can run without heavy
dependencies.  In a real system you would extend these functions to
interact with data warehouses, data lakes, data loss prevention
services, and more sophisticated anomaly detection algorithms.
"""

from __future__ import annotations

import json
import os
import re
import uuid
from typing import Any, Dict, List

import numpy as np
import pandas as pd


def create_datasets(base_dir: str = "data", n: int = 500) -> None:
    """Generate synthetic baseline and current datasets.

    Creates two CSV files in ``base_dir``:

    * ``transactions_baseline.csv`` – a clean dataset used as the
      baseline profile.
    * ``transactions_current.csv`` – a dataset with schema drift,
      anomalies, duplicate rows, and unmasked PII.

    Args:
        base_dir: Directory where the CSVs will be written.
        n: Number of records in the baseline dataset.  The current
           dataset may contain additional duplicates.
    """
    os.makedirs(base_dir, exist_ok=True)
    np.random.seed(42)
    data_baseline = {
        "transaction_id": np.arange(1, n + 1),
        "product_id": np.random.randint(100, 200, n),
        "region": np.random.choice(["North", "South", "East", "West"], n),
        "price": np.random.normal(loc=50, scale=5, size=n).round(2),
        "quantity": np.random.poisson(lam=3, size=n),
        "transaction_date": pd.date_range("2024-01-01", periods=n, freq="H"),
    }
    baseline_df = pd.DataFrame(data_baseline)
    baseline_df["customer_email"] = [f"cust{i}@example.com" for i in range(n)]
    baseline_df.to_csv(os.path.join(base_dir, "transactions_baseline.csv"), index=False)

    # Current dataset with drift and anomalies
    current_df = baseline_df.copy()
    current_df = current_df.rename(columns={"price": "unit_price"})
    current_df["discount"] = np.random.choice([0, 0.1, 0.2, 0.3], n)
    current_df.loc[current_df["region"] == "West", "unit_price"] *= 0.6
    duplicates = current_df.sample(10, random_state=1)
    current_df = pd.concat([current_df, duplicates], ignore_index=True)
    leaked_indices = current_df.sample(5, random_state=2).index
    current_df.loc[leaked_indices, "customer_email"] = [
        "alice@example.com",
        "bob@example.com",
        "charlie@example.com",
        "dave@example.com",
        "eve@example.com",
    ]
    current_df.to_csv(os.path.join(base_dir, "transactions_current.csv"), index=False)


def profile_data(file_path: str) -> Dict[str, Dict[str, float]]:
    """Build a simple profile of a CSV dataset.

    Returns a dictionary summarising the number of rows, column
    dtypes, missing values, and basic statistics for numeric columns.

    Args:
        file_path: Path to the CSV file.

    Returns:
        A nested dictionary keyed by column name.
    """
    df = pd.read_csv(file_path)
    profile: Dict[str, Dict[str, float]] = {"num_rows": int(len(df)), "columns": {}}
    for col in df.columns:
        col_info: Dict[str, float] = {
            "dtype": str(df[col].dtype),
            "num_missing": int(df[col].isna().sum()),
        }
        if pd.api.types.is_numeric_dtype(df[col]):
            col_info.update(
                {
                    "mean": float(df[col].mean()),
                    "std": float(df[col].std()),
                    "min": float(df[col].min()),
                    "max": float(df[col].max()),
                }
            )
        profile["columns"][col] = col_info
    return profile


def detect_anomalies_and_drift(current_path: str, baseline_profile: Dict[str, Dict[str, float]]) -> List[str]:
    """Compare current data against a baseline profile to identify issues.

    Detects added or missing columns, shifts in means greater than 30%,
    and duplicate rows.

    Args:
        current_path: Path to the current CSV file.
        baseline_profile: Profile generated by ``profile_data`` for the
            baseline dataset.

    Returns:
        A list of human‑readable descriptions of detected anomalies.
    """
    anomalies: List[str] = []
    current_df = pd.read_csv(current_path)
    baseline_cols = set(baseline_profile["columns"].keys())
    current_cols = set(current_df.columns)
    added_cols = current_cols - baseline_cols
    removed_cols = baseline_cols - current_cols
    if added_cols:
        anomalies.append(f"Schema drift detected: added columns {sorted(added_cols)}")
    if removed_cols:
        anomalies.append(f"Schema drift detected: missing columns {sorted(removed_cols)}")

    # Compare means for numeric columns present in both datasets
    for col in baseline_cols & current_cols:
        if pd.api.types.is_numeric_dtype(current_df[col]):
            baseline_mean = baseline_profile["columns"][col].get("mean")
            current_mean = current_df[col].mean()
            if baseline_mean is not None and baseline_mean != 0:
                change_ratio = abs(current_mean - baseline_mean) / abs(baseline_mean)
                if change_ratio > 0.3:
                    anomalies.append(
                        f"Distribution shift in {col}: baseline mean={baseline_mean:.2f}, current mean={current_mean:.2f}"
                    )

    duplicate_count = int(current_df.duplicated().sum())
    if duplicate_count > 0:
        anomalies.append(f"{duplicate_count} duplicate records detected")
    return anomalies


def detect_pii(file_path: str) -> List[str]:
    """Scan for unmasked email addresses and flag them as issues.

    The function inspects object‑type columns for values that match a
    simple email pattern.  If an email does not end with
    ``@example.com`` it is considered exposed PII for demo purposes.

    Args:
        file_path: Path to the CSV file.

    Returns:
        A list of strings describing compliance issues.
    """
    df = pd.read_csv(file_path)
    issues: List[str] = []
    email_regex = re.compile(r"[\w\.-]+@[\w\.-]+")
    for col in df.columns:
        if df[col].dtype == object:
            sample_values = df[col].dropna().astype(str).head(20)
            for val in sample_values:
                if email_regex.fullmatch(val):
                    if not val.endswith("@example.com"):
                        issues.append(f"Unmasked PII detected in column '{col}': {val}")
                    break
    return issues


def sync_tables(current_path: str) -> str:
    """Remove duplicate rows from the current dataset and write a cleaned copy.

    Args:
        current_path: Path to the CSV file to clean.

    Returns:
        The path to the cleaned CSV file.
    """
    df = pd.read_csv(current_path)
    cleaned_df = df.drop_duplicates()
    cleaned_path = current_path.replace(".csv", "_cleaned.csv")
    cleaned_df.to_csv(cleaned_path, index=False)
    return cleaned_path


def generate_lineage_graph() -> str:
    """Return a placeholder lineage description.

    In a real system this would produce a lineage graph relating
    sources to downstream tables.  The placeholder indicates the
    relationship between baseline, current, and cleaned datasets.

    Returns:
        A string describing lineage.
    """
    return "Lineage graph placeholder: baseline.csv → current.csv → cleaned.csv"


def save_governance_report(report: Dict[str, Any], output_dir: str = "data") -> str:
    """Write a governance report to disk in JSON format.

    Args:
        report: A dictionary of profiling, anomaly, compliance, and repair results.
        output_dir: Directory where the report should be saved.

    Returns:
        The path to the written report file.
    """
    os.makedirs(output_dir, exist_ok=True)
    file_name = f"governance_report_{uuid.uuid4().hex}.json"
    path = os.path.join(output_dir, file_name)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2)
    return path


# ---------------------------------------------------------------------------
# Helper to compile a governance report from component results
def compile_report(
    profile: Dict[str, Any],
    anomalies: List[str],
    compliance_issues: List[str],
    cleaned_path: str,
    lineage: str,
) -> Dict[str, Any]:
    """Compile results from various steps into a single report dict.

    Args:
        profile: The profile dictionary generated by :func:`profile_data`.
        anomalies: A list of anomaly descriptions from
            :func:`detect_anomalies_and_drift`.
        compliance_issues: A list of compliance issues from
            :func:`detect_pii`.
        cleaned_path: Path to the cleaned CSV file returned by
            :func:`sync_tables`.
        lineage: A lineage description returned by
            :func:`generate_lineage_graph`.

    Returns:
        A report dictionary with a timestamp and consolidated results.
    """
    import datetime as _dt

    return {
        "timestamp": _dt.datetime.utcnow().isoformat(),
        "profile": profile,
        "anomalies": anomalies,
        "compliance_issues": compliance_issues,
        "cleaned_path": cleaned_path,
        "lineage": lineage,
    }
